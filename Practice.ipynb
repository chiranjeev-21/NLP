{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Load the Treebank corpus\n",
        "nltk.download('treebank')\n",
        "corpus = nltk.corpus.treebank\n",
        "\n",
        "# Get all adjectives in the corpus\n",
        "adjectives = [word.lower() for (word, pos) in corpus.tagged_words() if pos.startswith('JJ')]\n",
        "\n",
        "# Calculate the average length of adjectives\n",
        "avg_length = sum(len(word) for word in adjectives) / len(adjectives)\n",
        "\n",
        "# Print the result\n",
        "print(\"The average length of adjectives in the Treebank corpus is:\", avg_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBX0oGv9Jy_p",
        "outputId": "003be176-7209-4a6a-a36c-318de9313239"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The average length of adjectives in the Treebank corpus is: 7.112709082382366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Load the Brown Corpus\n",
        "nltk.download('brown')\n",
        "corpus = nltk.corpus.brown\n",
        "\n",
        "# Find all words in the corpus that have exactly 5 characters and start with a vowel\n",
        "filtered_words = [word.lower() for word in corpus.words() if len(word) == 5 and word[0] in ['a', 'e', 'i', 'o', 'u']]\n",
        "\n",
        "# Calculate the average frequency of these words in the corpus\n",
        "freq_dist = nltk.FreqDist(filtered_words)\n",
        "total_freq = sum(freq_dist.values())\n",
        "num_words = len(freq_dist)\n",
        "avg_freq = total_freq / num_words\n",
        "\n",
        "# Print the results\n",
        "print(\"Average frequency of words in the Brown Corpus that have exactly 5 characters and start with a vowel:\", avg_freq)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIV_IUlsQd8J",
        "outputId": "3ebf629b-89e1-42b2-dd22-f5015a3e6584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average frequency of words in the Brown Corpus that have exactly 5 characters and start with a vowel: 40.33704735376045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Load the Reuters Corpus\n",
        "nltk.download('reuters')\n",
        "corpus = nltk.corpus.reuters\n",
        "\n",
        "# # Define a list of words to look for after \"president\"\n",
        "# search_words = [\"of\", \"and\", \"is\", \"was\", \"has\", \"will\", \"said\", \"reagan\", \"bush\", \"clinton\", \"obama\", \"trump\"]\n",
        "\n",
        "# Create a conditional frequency distribution of words after \"president\"\n",
        "cfd = nltk.ConditionalFreqDist(\n",
        "    (prev_word.lower(), word.lower())\n",
        "    for prev_word, word in nltk.bigrams(corpus.words())\n",
        "    if prev_word.lower() == 'president' \n",
        ")\n",
        "\n",
        "# Print the most common words after \"president\"\n",
        "print(\"Most common words after 'president':\")\n",
        "for word in cfd['president'].most_common():\n",
        "    print(word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3X_qDkAlSv17",
        "outputId": "a52d6bb5-a720-4f6a-8422-e84625f4c188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common words after 'president':\n",
            "('of', 177)\n",
            "('reagan', 123)\n",
            "('and', 87)\n",
            "(',', 39)\n",
            "('karl', 24)\n",
            "(\"'\", 12)\n",
            "('corazon', 10)\n",
            "('at', 10)\n",
            "('.', 10)\n",
            "('jorio', 10)\n",
            "('suharto', 10)\n",
            "('ronald', 9)\n",
            "('rilwanu', 9)\n",
            "('said', 9)\n",
            "('kenneth', 8)\n",
            "('for', 8)\n",
            "('helmut', 8)\n",
            "('jose', 8)\n",
            "('ferdinand', 7)\n",
            "('richard', 7)\n",
            "('michael', 6)\n",
            "('barber', 6)\n",
            "('to', 6)\n",
            "('jacques', 6)\n",
            "('alan', 6)\n",
            "('-', 6)\n",
            "('william', 5)\n",
            "('james', 5)\n",
            "('jaime', 5)\n",
            "('victor', 5)\n",
            "('dieter', 4)\n",
            "('jean', 4)\n",
            "('is', 4)\n",
            "('francois', 4)\n",
            "('felix', 4)\n",
            "('in', 4)\n",
            "('leon', 4)\n",
            "('saddam', 4)\n",
            "('peter', 3)\n",
            "('yoweri', 3)\n",
            "('wim', 3)\n",
            "('george', 3)\n",
            "('edward', 3)\n",
            "('kjell', 3)\n",
            "('paul', 3)\n",
            "('donald', 3)\n",
            "('andres', 3)\n",
            "('schlesinger', 2)\n",
            "('rosemary', 2)\n",
            "('tom', 2)\n",
            "('morton', 2)\n",
            "('douglas', 2)\n",
            "('ali', 2)\n",
            "('says', 2)\n",
            "('daniel', 2)\n",
            "('nobuo', 2)\n",
            "('told', 2)\n",
            "('nicholas', 2)\n",
            "('authorised', 2)\n",
            "('joao', 2)\n",
            "('robert', 2)\n",
            "('sees', 2)\n",
            "('marc', 2)\n",
            "('stefan', 2)\n",
            "('lines', 2)\n",
            "('valery', 2)\n",
            "('c', 2)\n",
            "('lane', 2)\n",
            "('.\"', 2)\n",
            "('yukuo', 2)\n",
            "('samuel', 2)\n",
            "('koh', 1)\n",
            "('j', 1)\n",
            "('sennen', 1)\n",
            "('louis', 1)\n",
            "('m', 1)\n",
            "('pierre', 1)\n",
            "('pedro', 1)\n",
            "('bjorn', 1)\n",
            "('chun', 1)\n",
            "('frank', 1)\n",
            "('kaunda', 1)\n",
            "('bruce', 1)\n",
            "('harold', 1)\n",
            "('(', 1)\n",
            "('had', 1)\n",
            "('t', 1)\n",
            "('carol', 1)\n",
            "('has', 1)\n",
            "('f', 1)\n",
            "('lukman', 1)\n",
            "('covering', 1)\n",
            "('jimmy', 1)\n",
            "('nixon', 1)\n",
            "('r', 1)\n",
            "('elias', 1)\n",
            "('or', 1)\n",
            "('hans', 1)\n",
            "('cecil', 1)\n",
            "('maurizio', 1)\n",
            "('max', 1)\n",
            "('hernan', 1)\n",
            "('brett', 1)\n",
            "('added', 1)\n",
            "('would', 1)\n",
            "('francisco', 1)\n",
            "('jerome', 1)\n",
            "('wang', 1)\n",
            "('hissene', 1)\n",
            "('nerio', 1)\n",
            "('leopoldo', 1)\n",
            "('remains', 1)\n",
            "('was', 1)\n",
            "('weakened', 1)\n",
            "('joseph', 1)\n",
            "('a', 1)\n",
            "('heinrich', 1)\n",
            "('favored', 1)\n",
            "('fred', 1)\n",
            "('fidel', 1)\n",
            "('franco', 1)\n",
            "('howard', 1)\n",
            "('ike', 1)\n",
            "('bert', 1)\n",
            "('patrick', 1)\n",
            "('with', 1)\n",
            "('alton', 1)\n",
            "('lorne', 1)\n",
            "('darwin', 1)\n",
            "('orville', 1)\n",
            "('general', 1)\n",
            "('dean', 1)\n",
            "('edson', 1)\n",
            "('raymond', 1)\n",
            "('dam', 1)\n",
            "('earl', 1)\n",
            "('owen', 1)\n",
            "('febres', 1)\n",
            "('charles', 1)\n",
            "('sheikh', 1)\n",
            "('yasuoki', 1)\n",
            "('abdul', 1)\n",
            "('praised', 1)\n",
            "('janos', 1)\n",
            "('warren', 1)\n",
            "('&', 1)\n",
            "('cos', 1)\n",
            "('brian', 1)\n",
            "('yotaro', 1)\n",
            "('klaus', 1)\n",
            "('jerry', 1)\n",
            "('gustave', 1)\n",
            "('jier', 1)\n",
            "('defends', 1)\n",
            "('hosni', 1)\n",
            "('anwar', 1)\n",
            "('may', 1)\n",
            "('harvey', 1)\n",
            "('carl', 1)\n",
            "('leo', 1)\n",
            "('joe', 1)\n",
            "('should', 1)\n",
            "('seeking', 1)\n",
            "('marvin', 1)\n",
            "('glenn', 1)\n",
            "('jim', 1)\n",
            "('lee', 1)\n",
            "('norman', 1)\n",
            "('rodolfo', 1)\n",
            "('augusto', 1)\n",
            "('marcel', 1)\n",
            "('arthur', 1)\n",
            "('sant', 1)\n",
            "('waree', 1)\n",
            "('steven', 1)\n",
            "('leaves', 1)\n",
            "('frederick', 1)\n",
            "('javier', 1)\n",
            "('ictor', 1)\n",
            "('responsible', 1)\n",
            "('lon', 1)\n",
            "('weizsaecker', 1)\n",
            "('philippe', 1)\n",
            "('jack', 1)\n",
            "('juan', 1)\n",
            "('pablo', 1)\n",
            "('tony', 1)\n",
            "('kurt', 1)\n",
            "('visits', 1)\n",
            "('alex', 1)\n",
            "('dave', 1)\n",
            "('not', 1)\n",
            "('ira', 1)\n",
            "('wolfgang', 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Load the WebText Corpus\n",
        "nltk.download('webtext')\n",
        "corpus = nltk.corpus.webtext\n",
        "# print(corpus.fileids)\n",
        "# Define the search term\n",
        "search_term = 'Web'\n",
        "\n",
        "# Find all trigrams that contain the search term\n",
        "trigrams = [trigram for trigram in nltk.ngrams(corpus.words(), 3) if search_term in trigram]\n",
        "\n",
        "# Calculate the frequency distribution of the trigrams\n",
        "freq_dist = nltk.FreqDist(trigrams)\n",
        "\n",
        "# Print the most frequent trigrams\n",
        "print(\"Most frequent trigrams that contain the word 'affection':\")\n",
        "for trigram in freq_dist.most_common(10):\n",
        "    print(trigram)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96W1peu4TOsP",
        "outputId": "f36722b2-8adc-4a11-f7bb-ab499dab02e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Package webtext is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most frequent trigrams that contain the word 'affection':\n",
            "(('originating', 'Web', 'site'), 5)\n",
            "(('Web', 'site', 'only'), 5)\n",
            "(('the', 'originating', 'Web'), 4)\n",
            "(('Outlook', 'Web', 'Access'), 3)\n",
            "(('MS', 'Outlook', 'Web'), 2)\n",
            "(('Web', 'Features', ':'), 2)\n",
            "(('organize', 'by', 'Web'), 2)\n",
            "(('by', 'Web', 'sites'), 2)\n",
            "(('Web', 'Search', '('), 2)\n",
            "(('Web', 'Search', 'for'), 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import webtext\n",
        "\n",
        "# Load the webtext corpus\n",
        "corpus = webtext.words()\n",
        "\n",
        "# Define a function to extract trigrams that contain the word \"affection\"\n",
        "def get_affection_trigrams(words):\n",
        "    trigrams = nltk.trigrams(words)\n",
        "    return [tri for tri in trigrams if 'affection' in tri]\n",
        "\n",
        "# Get the trigrams that contain the word \"affection\"\n",
        "affection_trigrams = get_affection_trigrams(corpus)\n",
        "\n",
        "# Count the frequency of each trigram\n",
        "freq_dist = nltk.FreqDist(affection_trigrams)\n",
        "\n",
        "# Print the 20 most frequent trigrams\n",
        "print(\"20 most frequent trigrams that contain the word 'affection':\")\n",
        "for tri, freq in freq_dist.most_common(20):\n",
        "    print(' '.join(tri), freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnQ0Fej-Twbx",
        "outputId": "e0536f9f-1f11-44fe-be04-a919591acbf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20 most frequent trigrams that contain the word 'affection':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "\n",
        "text = \"I was born on 21 January 2003, You were born on 31 May 2020\"\n",
        "date_pattern = r'\\d{1,2} [A-Za-z]+ \\d{4}'\n",
        "birth_dates = re.findall(date_pattern, text)\n",
        "total_count = len(birth_dates)\n",
        "\n",
        "print(\"Birth dates:\", birth_dates)\n",
        "print(\"Total count:\", total_count)\n"
      ],
      "metadata": {
        "id": "LYLjgyXEUZ83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11194907-e204-44fe-b1d9-5b9a0c57ff82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Birth dates: ['21 January 2003', '31 May 2020']\n",
            "Total count: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "text = \"In this code, we first import the re module for regular expressions and the nltk module for natural language processing. Then, we define a regular expression pattern date_pattern that matches a date in the format of one or two digits, followed by a space, followed by one or more alphabetical characters, followed by a space, followed by four digits. We then use the re.findall() function to extract all matches of this pattern in the text, which gives us a list of birth dates. Finally, we use the len() function to count the number of elements in this list, which gives us the total count of birth dates.\"\n",
        "\n",
        "# Download stopwords if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Get a set of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Count the number of stop words in the text\n",
        "stop_words_count = len([token for token in tokens if token.lower() in stop_words])\n",
        "\n",
        "# Calculate the percentage of stop words to all tokens\n",
        "stop_words_percentage = (stop_words_count / len(tokens)) * 100\n",
        "\n",
        "print(\"Stop words count:\", stop_words_count)\n",
        "print(\"Stop words percentage:\", round(stop_words_percentage, 2), \"%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHH9TzrSh2rK",
        "outputId": "9db99246-3f93-4017-d366-f9c8191ce616"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop words count: 48\n",
            "Stop words percentage: 38.71 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Get a list of sentences from the Gutenberg Corpus\n",
        "sentences = gutenberg.sents()\n",
        "\n",
        "# Define a list of noun words that end with the letter 's'\n",
        "noun_words = [word.lower() for (word, tag) in nltk.pos_tag(gutenberg.words()) if tag.startswith('NN') and word.endswith('s')]\n",
        "\n",
        "# Create a frequency distribution of noun words that end with the letter 's'\n",
        "fdist = nltk.FreqDist(noun_words)\n",
        "\n",
        "# Print the 10 most common noun words that end with the letter 's'\n",
        "print(\"The 10 most common noun words that end with the letter 's':\")\n",
        "for word, frequency in fdist.most_common(10):\n",
        "    print(word, frequency)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DoRoUASil6P",
        "outputId": "30114557-b515-4a4d-f205-ef7d69819d86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 10 most common noun words that end with the letter 's':\n",
            "s 4745\n",
            "things 1975\n",
            "mrs 1851\n",
            "eyes 1418\n",
            "days 1269\n",
            "sons 1191\n",
            "miss 1131\n",
            "words 1055\n",
            "years 1023\n",
            "hands 981\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Load the movie_reviews corpus\n",
        "nltk.download('movie_reviews')\n",
        "corpus = nltk.corpus.movie_reviews\n",
        "\n",
        "# Find all words in the corpus that contain at least 1 exclamation mark\n",
        "filtered_words = [word.lower() for word in corpus.words() if '!' in word]\n",
        "\n",
        "# Calculate the frequency distribution of these words in the corpus\n",
        "freq_dist = nltk.FreqDist(filtered_words)\n",
        "total_freq = sum(freq_dist.values())\n",
        "num_words = len(freq_dist)\n",
        "avg_freq = total_freq / num_words\n",
        "\n",
        "# Print the results\n",
        "print(\"Average frequency of words in the movie_reviews corpus that contain at least 1 exclamation mark:\", avg_freq)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imKrbN0bjSdR",
        "outputId": "53baf4da-69fa-4718-893a-50a412a393fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average frequency of words in the movie_reviews corpus that contain at least 1 exclamation mark: 1713.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Load the Brown Corpus and get the list of genres\n",
        "nltk.download('brown')\n",
        "genres = nltk.corpus.brown.categories()\n",
        "\n",
        "# Define a function to calculate lexical diversity\n",
        "def lexical_diversity(text):\n",
        "    return len(set(text)) / len(text)\n",
        "\n",
        "# Generate a table of lexical diversity scores for each genre\n",
        "print(\"Genre\\t\\tLexical Diversity\")\n",
        "print(\"----------------------------------\")\n",
        "for genre in genres:\n",
        "    words = nltk.corpus.brown.words(categories=genre)\n",
        "    lexical_div = lexical_diversity(words)\n",
        "    print(\"{}\\t\\t{:.4f}\".format(genre, lexical_div))\n",
        "\n",
        "# Find the genre with the lowest diversity\n",
        "lowest_div_genre = min(genres, key=lambda genre: lexical_diversity(nltk.corpus.brown.words(categories=genre)))\n",
        "print(\"\\n{} has the lowest lexical diversity.\".format(lowest_div_genre))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12mqBNxejwXq",
        "outputId": "4bec1b8c-822b-40d5-a8f4-89106fdf2730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Genre\t\tLexical Diversity\n",
            "----------------------------------\n",
            "adventure\t\t0.1280\n",
            "belles_lettres\t\t0.1064\n",
            "editorial\t\t0.1605\n",
            "fiction\t\t0.1358\n",
            "government\t\t0.1167\n",
            "hobbies\t\t0.1449\n",
            "humor\t\t0.2313\n",
            "learned\t\t0.0927\n",
            "lore\t\t0.1315\n",
            "mystery\t\t0.1221\n",
            "news\t\t0.1431\n",
            "religion\t\t0.1618\n",
            "reviews\t\t0.2119\n",
            "romance\t\t0.1207\n",
            "science_fiction\t\t0.2234\n",
            "\n",
            "learned has the lowest lexical diversity.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Load the Brown Corpus and get the list of words in the news genre\n",
        "nltk.download('brown')\n",
        "news_words = nltk.corpus.brown.words(categories='news')\n",
        "\n",
        "# Define a function to check if a word starts with \"wh\"\n",
        "def starts_with_wh(word):\n",
        "    return word.lower().startswith('wh')\n",
        "\n",
        "# Count the number of words in the news genre that start with \"wh\"\n",
        "wh_words = [word for word in news_words if starts_with_wh(word)]\n",
        "num_wh_words = len(wh_words)\n",
        "\n",
        "# Print the results\n",
        "print(\"Number of words in the 'news' genre of the Brown Corpus that start with 'wh':\", num_wh_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwBRkE_jlazf",
        "outputId": "1615b425-23b3-4522-fea6-a11d647699c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words in the 'news' genre of the Brown Corpus that start with 'wh': 1062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Load the Inaugural Address Corpus and get the fileids\n",
        "nltk.download('inaugural')\n",
        "fileids = nltk.corpus.inaugural.fileids()\n",
        "\n",
        "# Extract the year from each filename\n",
        "years = [int(fileid[:4]) for fileid in fileids]\n",
        "\n",
        "# Print the list of years\n",
        "print(\"Years in the Inaugural Address Corpus:\")\n",
        "print(years)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v21hbMaLltje",
        "outputId": "7ece803c-1e79-4671-bfef-0af7f8900797"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Years in the Inaugural Address Corpus:\n",
            "[1789, 1793, 1797, 1801, 1805, 1809, 1813, 1817, 1821, 1825, 1829, 1833, 1837, 1841, 1845, 1849, 1853, 1857, 1861, 1865, 1869, 1873, 1877, 1881, 1885, 1889, 1893, 1897, 1901, 1905, 1909, 1913, 1917, 1921, 1925, 1929, 1933, 1937, 1941, 1945, 1949, 1953, 1957, 1961, 1965, 1969, 1973, 1977, 1981, 1985, 1989, 1993, 1997, 2001, 2005, 2009, 2013, 2017, 2021]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/inaugural.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "txt = \"SpaCy allows users to update the model to include new examples with existing entities. SpaCy provides a pipeline component called ‘ner’ that finds token spans that match entities.\"\n",
        "\n",
        "# tokens=[]\n",
        "# tokens=txt.split(' ')\n",
        "doc = nlp(txt)\n",
        "\n",
        "## Named Entity Recognition\n",
        "for ent in doc.ents:\n",
        "    print(ent.text,ent.label_,ent.label_)\n",
        "\n",
        "print(\"\\n\")\n",
        "##POS Tagging\n",
        "for token in doc:\n",
        "   print(token,\"       \",token.tag_,\"       \",spacy.explain(token.tag_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RM1thL6Bmbeu",
        "outputId": "908bff8f-18ba-4a80-991a-c3043a5d520a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpaCy PERSON PERSON\n",
            "SpaCy PERSON PERSON\n",
            "\n",
            "\n",
            "SpaCy         NNP         noun, proper singular\n",
            "allows         VBZ         verb, 3rd person singular present\n",
            "users         NNS         noun, plural\n",
            "to         TO         infinitival \"to\"\n",
            "update         VB         verb, base form\n",
            "the         DT         determiner\n",
            "model         NN         noun, singular or mass\n",
            "to         TO         infinitival \"to\"\n",
            "include         VB         verb, base form\n",
            "new         JJ         adjective (English), other noun-modifier (Chinese)\n",
            "examples         NNS         noun, plural\n",
            "with         IN         conjunction, subordinating or preposition\n",
            "existing         VBG         verb, gerund or present participle\n",
            "entities         NNS         noun, plural\n",
            ".         .         punctuation mark, sentence closer\n",
            "SpaCy         NNP         noun, proper singular\n",
            "provides         VBZ         verb, 3rd person singular present\n",
            "a         DT         determiner\n",
            "pipeline         NN         noun, singular or mass\n",
            "component         NN         noun, singular or mass\n",
            "called         VBN         verb, past participle\n",
            "‘         ``         opening quotation mark\n",
            "ner         NN         noun, singular or mass\n",
            "’         ''         closing quotation mark\n",
            "that         WDT         wh-determiner\n",
            "finds         VBZ         verb, 3rd person singular present\n",
            "token         JJ         adjective (English), other noun-modifier (Chinese)\n",
            "spans         NNS         noun, plural\n",
            "that         WDT         wh-determiner\n",
            "match         NN         noun, singular or mass\n",
            "entities         NNS         noun, plural\n",
            ".         .         punctuation mark, sentence closer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "koGvSyqBnOI1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}